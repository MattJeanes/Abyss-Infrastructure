name: Setup

on:
  workflow_dispatch:
    inputs:
      planOnly:
        description: 'Terraform plan only (do not apply)'
        required: true
        default: 'false'

jobs:
  setup:
    name: Setup
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3

      - name: HashiCorp - Setup Terraform
        uses: hashicorp/setup-terraform@v2.0.0
        with:
          terraform_wrapper: false
        
      - name: Terraform Init
        run: terraform init -backend-config="storage_account_name=abyssterraform" -backend-config="container_name=terraform" -backend-config="key=terraform.tfstate" -backend-config="access_key=${{ secrets.TERRAFORM_STORAGE_ACCOUNT_KEY }}" -upgrade
        working-directory: terraform
        
      - name: Set Terraform variables
        run: |
          tee terraform.tfvars <<EOF
          kubernetes_version = "${{ secrets.KUBERNETES_VERSION }}"
          ssh_public_key = "${{ secrets.SSH_PUBLIC_KEY }}"
          cloudflare_api_token = "${{ secrets.CLOUDFLARE_API_TOKEN }}"
          cloudflare_zone_id = "${{ secrets.CLOUDFLARE_ZONE_ID }}"
          home_ip = "${{ secrets.HOME_IP }}"
          EOF
        working-directory: terraform
        
      - name: Terraform Plan
        run: terraform plan -out tfplan
        working-directory: terraform
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      
      - name: Terraform Validate and Apply
        run: terraform apply tfplan
        working-directory: terraform
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        if: github.event.inputs.planOnly == 'false'

      - name: Retrieve terraform outputs
        run: |
          $outputs = terraform output -json | ConvertFrom-Json -AsHashTable
          $outputs.Keys | ForEach-Object {
              $key = $_
              $output = $outputs[$key]
              $value = $output.value
              $sensitive = $output.sensitive
              
              Write-Host "$key = $(($sensitive ? "***" : $value))"
              if ($sensitive) {
                Write-Host "::add-mask::$value"
              }
              "$key=$value" >> $env:GITHUB_ENV
          }
        shell: pwsh
        working-directory: terraform

      - name: Azure Login
        uses: Azure/login@v1.4.3
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Login to Kubernetes
        uses: Azure/aks-set-context@v2.1
        with:
          resource-group: abyss
          cluster-name: abyss

      - name: Set common variables
        run: |
          $variables = @{
            hello_world_host = "hello-world.${{ secrets.HOST }}"
            dashboard_host = "kubedashboard.${{ secrets.HOST }}"
            alertmanager_host = "alertmanager.${{ secrets.HOST }}"
            grafana_host = "newgrafana.${{ secrets.HOST }}"
            prometheus_host = "prometheus.${{ secrets.HOST }}"
            auth_host = "auth.${{ secrets.HOST }}"
            auth_url = "http://oauth2-proxy.oauth2-proxy.svc.cluster.local/oauth2/auth"
            auth_signin = "https://auth.${{ secrets.HOST }}/oauth2/start?rd=`$scheme://`$host`$request_uri"
          }
          $variables.Keys | ForEach-Object {
              $key = $_
              $value = $variables[$key]
              Write-Host "$key = $value"
              "$key=$value" >> $env:GITHUB_ENV
          }
        shell: pwsh

      - name: Install storage-account-secret
        run: |
          helm upgrade --install --wait --atomic --debug \
            storage-account-secret ./storage-account-secret \
            --namespace default \
            --set storageAccountName="${{ env.storage_account_name }}" \
            --set storageAccountKey="${{ env.storage_account_key }}"
        working-directory: kubernetes/charts
      
      # https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx
      - name: Install ingress-nginx
        run: |
          helm upgrade --install --wait --atomic --debug \
            ingress-nginx ingress-nginx \
            --repo https://kubernetes.github.io/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --version 4.1.4 \
            -f ingress-nginx.yaml \
            --set controller.service.loadBalancerIP="${{ env.public_ip_address }}"
        working-directory: kubernetes/releases

      # https://artifacthub.io/packages/helm/cert-manager/cert-manager
      - name: Install cert-manager
        run: |
          helm upgrade --install --wait --atomic --debug \
            cert-manager cert-manager \
            --repo https://charts.jetstack.io \
            --namespace cert-manager \
            --create-namespace \
            --version 1.8.0 \
            -f cert-manager.yaml
        working-directory: kubernetes/releases

      - name: Install certificate-issuers
        run: |
          helm upgrade --install --wait --atomic --debug \
            certificate-issuers ./certificate-issuers \
            --namespace cert-manager \
            --set letsEncrypt.email="${{ secrets.EMAIL }}"
        working-directory: kubernetes/charts

      - name: Install Hello World
        run: |
          helm upgrade --install --wait --atomic --debug \
            hello-world ./hello-world \
            --namespace default \
            --set host="${{ env.hello_world_host }}"
        working-directory: kubernetes/charts

      - name: Wait for hello world certificate to be ready
        run: ./WaitKubeCertificate.ps1 -Name "hello-world-tls"
        shell: pwsh
        working-directory: scripts

      # https://artifacthub.io/packages/helm/oauth2-proxy/oauth2-proxy
      - name: Install oauth2-proxy
        run: |
          helm upgrade --install --wait --atomic --debug \
            oauth2-proxy oauth2-proxy \
            --repo https://oauth2-proxy.github.io/manifests \
            --namespace oauth2-proxy \
            --create-namespace \
            --version 6.2.1 \
            -f oauth2-proxy.yaml \
            --set config.clientID="${{ secrets.OAUTH2_CLIENT_ID }}" \
            --set config.clientSecret="${{ secrets.OAUTH2_CLIENT_SECRET }}" \
            --set config.cookieSecret="${{ secrets.OAUTH2_COOKIE_SECRET }}" \
            --set authenticatedEmailsFile.restricted_access="${{ secrets.EMAIL }}" \
            --set ingress.hosts[0]="${{ env.auth_host }}" \
            --set ingress.tls[0].hosts[0]="${{ env.auth_host }}" \
            --set extraArgs.whitelist-domain=".${{ secrets.HOST }}" \
            --set extraArgs.cookie-domain=".${{ secrets.HOST }}"
        working-directory: kubernetes/releases
      
      - name: Wait for oauth2-proxy certificate to be ready
        run: ./WaitKubeCertificate.ps1 -Name "oauth2-proxy" -Namespace "oauth2-proxy"
        shell: pwsh
        working-directory: scripts

      # https://artifacthub.io/packages/helm/fairwinds-stable/vpa
      - name: Install vpa
        run: |
          helm upgrade --install --wait --atomic --debug \
            vpa vpa \
            --repo https://charts.fairwinds.com/stable \
            --namespace vpa \
            --create-namespace \
            --version 1.4.0 \
            -f vpa.yaml
        working-directory: kubernetes/releases

      - name: Install persistent volume for prometheus
        run: |
          helm upgrade --install --wait --atomic --debug \
            prometheus-persistentvolume ./azure-persistentvolume \
            --namespace monitoring \
            --create-namespace \
            --set name=prometheus \
            --set disk.capacity=4Gi \
            --set disk.id="${{ env.prometheus_disk_id }}" \
            --set persistentVolumeClaim.enabled=false
        working-directory: kubernetes/charts

      - name: Install persistent volume for alertmanager
        run: |
          helm upgrade --install --wait --atomic --debug \
            alertmanager-persistentvolume ./azure-persistentvolume \
            --namespace monitoring \
            --create-namespace \
            --set name=alertmanager \
            --set disk.capacity=4Gi \
            --set disk.id="${{ env.alertmanager_disk_id }}" \
            --set persistentVolumeClaim.enabled=false
        working-directory: kubernetes/charts

      # https://artifacthub.io/packages/helm/oauth2-proxy/oauth2-proxy
      - name: Install prometheus
        run: |
          helm upgrade --install --wait --atomic --debug \
            kube-prometheus-stack kube-prometheus-stack \
            --repo https://prometheus-community.github.io/helm-charts \
            --namespace monitoring \
            --create-namespace \
            --version 36.0.1 \
            -f kube-prometheus-stack.yaml \
            --set alertmanager.ingress.hosts[0]="${{ env.alertmanager_host }}" \
            --set alertmanager.ingress.tls[0].hosts[0]="${{ env.alertmanager_host }}" \
            --set alertmanager.ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-url="${{ env.auth_url }}" \
            --set alertmanager.ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-signin="${{ env.auth_signin }}" \
            --set alertmanager.config.global.pagerduty_url="${{ secrets.PAGERDUTY_URL }}" \
            --set alertmanager.config.receivers[0].pagerduty_configs[0].routing_key="${{ secrets.PAGERDUTY_INTEGRATION_KEY }}" \
            --set alertmanager.config.receivers[1].webhook_configs[0].url="${{ secrets.DEAD_MANS_SNITCH_WEBHOOK_URL }}" \
            --set grafana.ingress.hosts[0]="${{ env.grafana_host }}" \
            --set grafana.ingress.tls[0].hosts[0]="${{ env.grafana_host }}" \
            --set grafana.adminPassword="${{ secrets.GRAFANA_ADMIN_PASSWORD }}" \
            --set prometheus.ingress.hosts[0]="${{ env.prometheus_host }}" \
            --set prometheus.ingress.tls[0].hosts[0]="${{ env.prometheus_host }}" \
            --set prometheus.ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-url="${{ env.auth_url }}" \
            --set prometheus.ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-signin="${{ env.auth_signin }}"
        working-directory: kubernetes/releases

      - name: Install prometheus-vpa
        run: |
          helm upgrade --install --wait --atomic --debug \
            prometheus-vpa ./prometheus-vpa \
            --namespace monitoring
        working-directory: kubernetes/charts

      - name: Wait for kube-prometheus-stack certificates to be ready
        run: |
          ./WaitKubeCertificate.ps1 -Name "prometheus-tls" -Namespace "monitoring"
          ./WaitKubeCertificate.ps1 -Name "alertmanager-tls" -Namespace "monitoring"
          ./WaitKubeCertificate.ps1 -Name "grafana-tls" -Namespace "monitoring"
        shell: pwsh
        working-directory: scripts
      
      - name: Create dashboard account
        run: |
          Write-Host "Creating namespace"
          kubectl create namespace dashboard --dry-run=client -o yaml | kubectl apply -f -
          if ($LASTEXITCODE -ne 0) {
              Write-Error "Failed to create namespace"
          }

          Write-Host "Creating service account"
          kubectl apply -f dashboard-service-account.yaml
          if ($LASTEXITCODE -ne 0) {
              Write-Error "Failed to create service account"
          }

          Write-Host "Creating cluster role binding"
          kubectl apply -f dashboard-cluster-role-binding.yaml
          if ($LASTEXITCODE -ne 0) {
              Write-Error "Failed to create cluster role binding"
          }

          Write-Host "Retrieving dashboard token"
          $dashboardSecrets = (kubectl get secret -n dashboard -o json | ConvertFrom-Json).items
          if ($LASTEXITCODE -ne 0) {
              Write-Error "Failed to get dashboard token"
          }
          $tokenBase64 = ($dashboardSecrets | Where-Object { $_.metadata.name.StartsWith("admin-user-token") }).data.token
          $token = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($tokenBase64))
          Write-Host "::add-mask::$token"
          "dashboard_token=$token" >> $env:GITHUB_ENV
        shell: pwsh
        working-directory: kubernetes/manifests

      # https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard
      - name: Install kubernetes-dashboard
        run: |
          helm upgrade --install --wait --atomic --debug \
            kubernetes-dashboard kubernetes-dashboard \
            --repo https://kubernetes.github.io/dashboard \
            --namespace dashboard \
            --create-namespace \
            --version 5.7.0 \
            -f kubernetes-dashboard.yaml \
            --set ingress.hosts[0]="${{ env.dashboard_host }}" \
            --set ingress.tls[0].hosts[0]="${{ env.dashboard_host }}" \
            --set ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/configuration-snippet="proxy_set_header Authorization \"Bearer ${{ env.dashboard_token }}\";" \
            --set ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-url="${{ env.auth_url }}"
            --set ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/auth-signin="${{ env.auth_signin }}"
        working-directory: kubernetes/releases

      - name: Wait for dashboard certificate to be ready
        run: ./WaitKubeCertificate.ps1 -Name "dashboard-tls" -Namespace "dashboard"
        shell: pwsh
        working-directory: scripts

      - name: Install persistent volume for mariadb
        run: |
          helm upgrade --install --wait --atomic --debug \
            mariadb-persistentvolume ./azure-persistentvolume \
            --namespace default \
            --set name=mariadb \
            --set disk.capacity=4Gi \
            --set disk.id="${{ env.mariadb_disk_id }}"
        working-directory: kubernetes/charts

      # https://artifacthub.io/packages/helm/bitnami/mariadb
      - name: Install mariadb
        run: |
          helm upgrade --install --wait --atomic --debug \
            mariadb mariadb \
            --repo https://charts.bitnami.com/bitnami \
            --namespace default \
            --version 11.0.3 \
            -f mariadb.yaml \
            --set auth.rootPassword="${{ secrets.MARIADB_ROOT_PASSWORD }}" \
            --set primary.service.loadBalancerIP="${{ env.public_ip_address }}"
        working-directory: kubernetes/releases

      - name: Install persistent volume for teamspeak
        run: |
          helm upgrade --install --wait --atomic --debug \
            teamspeak-persistentvolume ./azure-persistentvolume \
            --namespace default \
            --set name=teamspeak \
            --set disk.capacity=4Gi \
            --set disk.id="${{ env.teamspeak_disk_id }}"
        working-directory: kubernetes/charts

      # https://artifacthub.io/packages/helm/k8s-at-home/teamspeak
      - name: Install teamspeak
        run: |
          helm upgrade --install --wait --atomic --debug \
            teamspeak teamspeak \
            --repo https://k8s-at-home.com/charts \
            --namespace default \
            --version 0.5.2 \
            -f teamspeak.yaml \
            --set service.main.loadBalancerIP="${{ env.public_ip_address }}" \
            --set service.udp.loadBalancerIP="${{ env.public_ip_address }}" \
            --set env.TS3SERVER_SERVERADMIN_PASSWORD="${{ secrets.TEAMSPEAK_SERVERADMIN_PASSWORD }}"
            --set env.TS3SERVER_DB_PASSWORD="${{ secrets.TEAMSPEAK_DATABASE_PASSWORD }}"
        working-directory: kubernetes/releases