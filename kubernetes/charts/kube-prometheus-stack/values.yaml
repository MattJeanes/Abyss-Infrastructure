kube-prometheus-stack:
  crds:
    enabled: true
    upgradeJob:
      enabled: true
      forceConflicts: true
  prometheus-node-exporter:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: type
              operator: NotIn
              values:
              - virtual-kubelet
  alertmanager:
    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            volumeName: alertmanager
            resources:
              requests:
                storage: 4Gi
            accessModes:
            - ReadWriteOnce
            storageClassName: longhorn
    config:
      route:
        receiver: pagerduty
        routes:
        - matchers:
          - alertname = Watchdog
          receiver: deadmanssnitch
          repeat_interval: 15m
        - matchers:
          - alertname = InfoInhibitor
          receiver: "null"
      receivers:
      - name: pagerduty
      - name: deadmanssnitch
        webhook_configs:
        - send_resolved: false
      - name: "null"
      inhibit_rules:
      - equal:
        - namespace
        - alertname
        source_matchers:
        - severity = critical
        target_matchers:
        - severity =~ warning|info
      - equal:
        - namespace
        - alertname
        source_matchers:
        - severity = warning
        target_matchers:
        - severity = info
      - equal:
        - namespace
        source_matchers:
        - alertname = InfoInhibitor
        target_matchers:
        - severity = info
  grafana:
    persistence:
      enabled: true
      type: pvc
      existingClaim: grafana
    resources:
      requests:
        cpu: 50m
        memory: 300Mi
      limits:
        memory: 300Mi
    sidecar:
      dashboards:
        enabled: true
        searchNamespace: ALL
      datasources:
        enabled: true
        searchNamespace: ALL
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: letsencrypt-prod
      tls:
      - secretName: grafana-tls
    env:
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: panodata-map-panel
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
          memory: 800Mi
        limits:
          memory: 800Mi
      storageSpec:
        volumeClaimTemplate:
          spec:
            volumeName: prometheus
            resources:
              requests:
                storage: 10Gi
            accessModes:
            - ReadWriteOnce
            storageClassName: longhorn
      retention: 7d
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
  prometheusOperator:
    prometheusConfigReloader:
      resources:
        requests:
          cpu: 5m
          memory: 40Mi
        limits:
          cpu: "0"
          memory: 40Mi
  kubeScheduler:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeProxy:
    enabled: false
  defaultRules:
    disabled:
      TargetDown: true
      KubeAggregatedAPIErrors: true
  additionalPrometheusRulesMap:
    custom-rules:
      groups:
      - name: custom.rules
        rules:
        - alert: CPUUsageHigh
          annotations:
            description: '{{`{{`}} $value | humanizePercentage {{`}}`}} CPU usage relative to requests in namespace {{`{{`}} $labels.namespace {{`}}`}} for container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.pod {{`}}`}}.'
            summary: Container experienced elevated CPU usage relative to requests
          expr: |-
            sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate) by (namespace, pod, container)
              /
            (sum(kube_pod_container_resource_requests{resource="cpu", unit="core"}) by (namespace, pod, container))
              > ( 150 / 100 )
          for: 15m
          labels:
            severity: warning
        - alert: MemoryUsageHigh
          annotations:
            description: '{{`{{`}} $value | humanizePercentage {{`}}`}} memory usage relative to requests in namespace {{`{{`}} $labels.namespace {{`}}`}} for container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.pod {{`}}`}}.'
            summary: Container experienced elevated memory usage relative to requests
          expr: |-
            sum(node_namespace_pod_container:container_memory_working_set_bytes{namespace != "kube-system"}) by (namespace, pod, container)
              /
            (sum(kube_pod_container_resource_requests{namespace != "kube-system", resource="memory", unit="byte"}) by (namespace, pod, container))
              > ( 125 / 100 )
          for: 15m
          labels:
            severity: warning
      - name: longhorn.rules
        rules:
        - alert: LonghornVolumeActualSpaceUsedWarning
          annotations:
            description: The actual space used by Longhorn volume {{$labels.volume}} on {{$labels.node}} is at {{$value}}% capacity for
              more than 5 minutes.
            summary: The actual used space of Longhorn volume is over 95% of the capacity.
          expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 95
          for: 5m
          labels:
            issue: The actual used space of Longhorn volume {{$labels.volume}} on {{$labels.node}} is high.
            severity: warning
        - alert: LonghornVolumeStatusCritical
          annotations:
            description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Fault for
              more than 2 minutes.
            summary: Longhorn volume {{$labels.volume}} is Fault
          expr: longhorn_volume_robustness == 3
          for: 5m
          labels:
            issue: Longhorn volume {{$labels.volume}} is Fault.
            severity: critical
        - alert: LonghornVolumeStatusWarning
          annotations:
            description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Degraded for
              more than 5 minutes.
            summary: Longhorn volume {{$labels.volume}} is Degraded
          expr: longhorn_volume_robustness == 2
          for: 5m
          labels:
            issue: Longhorn volume {{$labels.volume}} is Degraded.
            severity: warning
        - alert: LonghornNodeStorageWarning
          annotations:
            description: The used storage of node {{$labels.node}} is at {{$value}}% capacity for
              more than 5 minutes.
            summary:  The used storage of node is over 95% of the capacity.
          expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 95
          for: 5m
          labels:
            issue: The used storage of node {{$labels.node}} is high.
            severity: warning
        - alert: LonghornDiskStorageWarning
          annotations:
            description: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is at {{$value}}% capacity for
              more than 5 minutes.
            summary:  The used storage of disk is over 95% of the capacity.
          expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 95
          for: 5m
          labels:
            issue: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is high.
            severity: warning
        - alert: LonghornNodeDown
          annotations:
            description: There are {{$value}} Longhorn nodes which have been offline for more than 5 minutes.
            summary: Longhorn nodes is offline
          expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
          for: 5m
          labels:
            issue: There are {{$value}} Longhorn nodes are offline
            severity: critical
        - alert: LonghornInstanceManagerCPUUsageWarning
          annotations:
            description: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is {{$value}}% for
              more than 5 minutes.
            summary: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is over 300%.
          expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
          for: 5m
          labels:
            issue: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} consumes 3 times the CPU request.
            severity: warning
        - alert: LonghornNodeCPUUsageWarning
          annotations:
            description: Longhorn node {{$labels.node}} has CPU Usage / CPU capacity is {{$value}}% for
              more than 5 minutes.
            summary: Longhorn node {{$labels.node}} experiences high CPU pressure for more than 5m.
          expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
          for: 5m
          labels:
            issue: Longhorn node {{$labels.node}} experiences high CPU pressure.
            severity: warning
      - name: adguardsync.rules
        rules:
        - alert: AdGuardSyncFailed
          annotations:
            description: '{{`{{`}} $labels.instance {{`}}`}} has not reported a successful AdGuard sync in the past 10 minutes or the sync failed.'
            summary: AdGuard sync failed or metric missing
          expr: |-
            absent_over_time(adguard_home_sync_sync_successful[10m]) or (adguard_home_sync_sync_successful == 0)
          for: 1m
          labels:
            severity: critical
        - alert: AdGuardSyncSlow
          annotations:
            description: 'AdGuard sync on {{`{{`}} $labels.instance {{`}}`}} has taken longer than 5 seconds consistently for 5 minutes.'
            summary: AdGuard sync is slow
          expr: |-
            adguard_home_sync_sync_duration_seconds > 5 and on(instance) adguard_home_sync_sync_successful
          for: 5m
          labels:
            severity: warning
  kube-state-metrics:
    resources:
      requests:
        cpu: 5m
        memory: 100Mi
      limits:
        memory: 100Mi